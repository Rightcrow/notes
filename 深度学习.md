# 深度学习



## 概率论与数理统计



#### 联合概率、边缘概率、条件概率

> - 联合概率P(X=a,Y=b)
>   满足X=a且Y=b的面积
> - 边缘概率P(X=a)
>   不考虑Y的取值，所有满足X=a的区域的总面积
> - 条件概率P(X=a|Y=b)
>   在Y=b的前提下，满足X=a的面积（比例）

![img](https://i.loli.net/2020/05/12/tsFICnL9QKkchHP.jpg)联

> 联合概率

$$
P(X=红色,Y=数字牌)=3/16
$$

> 边缘概率

$$
P(X=红色)=9/16
$$

> 条件概率

$$
P(Y=数字牌|X=红色)=3/9=1/3
$$



#### 似然概率

> 对于函数![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))，从不同的观测角度来看可以分为以下两种情况：
>
> - 如果![\theta](https://math.jianshu.com/math?formula=%5Ctheta)已知且保持不变，![x](https://math.jianshu.com/math?formula=x)是变量，则![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))称为概率函数，表示不同![x](https://math.jianshu.com/math?formula=x)出现的概率。
> - 如果![x](https://math.jianshu.com/math?formula=x)已知且保持不变，![\theta](https://math.jianshu.com/math?formula=%5Ctheta)是变量，则![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))称为似然函数，表示不同![\theta](https://math.jianshu.com/math?formula=%5Ctheta)下，![x](https://math.jianshu.com/math?formula=x)出现的概率，也记作![L(\theta|x)](https://math.jianshu.com/math?formula=L(%5Ctheta%7Cx))或![L(x;\theta)](https://math.jianshu.com/math?formula=L(x%3B%5Ctheta))或![f(https://math.jianshu.com/math?formula=f(x%3B\theta))](https://math.jianshu.com/math?formula=f(x%3B%5Ctheta))。





















## 统计学方法

### 第1章 统计学习及监督 学习概论

#### 统计学的方法（监督、无监督）

> 组成：监督学习supervised learning，无监督学习unsupervised learning，强化学习reinforcement learning

> 过程：
>
> 1. 给定数据，假设数据独立同分布；
> 2. 假设学习模型属于某函数集合（假设空间）；
> 3. 应用评价标准，在假设空间中选取最优模型；

> 三要素：模型model，策略strategy，算法algorithm
>
> 步骤：1. 确定学习模型集合  2. 确定策略（模型选择）方法  3. 用算法求解最优模型



#### 监督学习（变量连续、离散）

> 通过标注数据数据，给出输入输出的关系
>
> 本质：学习输入到输出的映射的统计规律

> 输入变量与输出变量连续———称为回归问题
>
> 输出变量为有限个离散变量———称为分类问题
>
> 输入和输出变量都为变量序列的预测———称为标注问题



#### 模型分类（生成模型、判别模型）

> 概率模型取条件概率分布形式

$$
P(Y/X)
$$

> 非概率模型取函数形式

$$
y=f(x)
$$

> 概率模型为联合概率分布，称为生成模型
>
> 非概率模型为条件概率分布或决策函数，称为判别模型

> 假设有四个samples： 

![img](https://i.loli.net/2020/05/11/riMRBUekbvhqO21.jpg)



> 生成式模型的世界是这个样子：朴素贝叶斯法和隐马尔可夫模型

![img](https://i.loli.net/2020/05/11/9FEgT4Lks8AylSa.jpg)
![[公式]](https://www.zhihu.com/equation?tex=\Sigma+P(x%2C+y)+%3D+1)

```
## 数据学习p(x,y)联合概率分布，从而求出p(y|x)的条件概率分布作为预测模型
```



> 而判定式模型的世界是这个样子：k近邻、感知机、决策树、逻辑回归、最大熵、支持向量机、提升方法、条件随机场
>
> 在x成立的条件下，y的概率

![img](https://i.loli.net/2020/05/11/FqmxQ7SCzZKAljc.jpg)![[公式]](https://www.zhihu.com/equation?tex=\sum_{y}{P(y+|+x)}+%3D+1+)

```
## 直接学习决策函数y=f(x)或者条件概率分布P(Y|X)作为预测模型
```



#### 算法分类

> 在线学习：每次接受一个样本，预测，学习模型，不断重复
>
> 批量学习：一次接受所有样本，学习模型，进行预测

<img src="https://i.loli.net/2020/05/11/bvH2xo9BAhl5rCq.png" alt="image-20200511154313619" style="zoom:50%;" />



#### 技巧分类（贝叶斯、极大似然、核方法）

> 贝叶斯学习
>
> 朴素贝叶斯、潜在狄利克雷分配

<img src="https://i.loli.net/2020/05/11/C3fniYuqWQXVo2g.png" alt="image-20200511155758680" style="zoom: 50%;" />

> 核方法
>
> 核PCA、核k均值、核函数支持向量机

> **把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。**



#### 假设空间（策略在此中选取最优模型）

> 决策函数表示的为非概率模型
>
> 条件概率表示的为概率模型

![image-20200512213925671](https://i.loli.net/2020/05/12/TzASwVu4EFB5KGC.png)



#### 策略（期望损失、经验损失）

> 损失函数度量模型一次预测好坏
>
> 风险函数度量平均意义下模型好坏

<img src="https://i.loli.net/2020/05/12/eFupKy1ioDxr8hG.png" alt="image-20200512214819190" style="zoom:50%;" />

<img src="C:%5CUsers%5Czhang%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200512214931661.png" alt="image-20200512214931661" style="zoom:50%;" />



> 联合分布P(x,y)未知，假设P(x,y)已知就可以直接求出P(y|x)，没必要学习

![image-20200512215656217](https://i.loli.net/2020/05/12/Cc4wWOqva7ztn2r.png)



> 再加上正则化项
>
> 监督学习问题称为经验函数的最优化问题

![image-20200512220028657](https://i.loli.net/2020/05/12/wpRSVcQX9WajzAv.png)



#### 泛化误差上界

> 样本越大泛化误差上界趋于0
>
> 假设空间越大，泛化误差上界越大



#### 生成模型

> 通过联合概率分布p(x,y)，求出条件概率分布p(y|x)作为预测模型
>
> 生成模型如下：

![image-20200515211114721](https://i.loli.net/2020/05/15/vjCF1PQVlLpaYDg.png)

```
# 优点：1.可以还原联合分布。
       2.收敛快，样本容量增加更快收敛于真实模型。
       3.存在隐变量时，仍可以用，但判别模型不行。
```



#### 判别模型

> 直接学习决策函数或者条件概率作为预测模型
>
> 判别方法关心，给定x，应该预测怎样的y



#### 监督学习问题（分类、标注、回归；精确率、召回率）

> 分类问题
>
> 输出Y为有限个离散值
>
> 从数据中学习一个分类模型或决策函数，称为分类器

<img src="https://i.loli.net/2020/05/16/MVwN9a4CtqT1Odn.png" alt="image-20200516102436786" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/16/4JYTNdePgk5vKOi.png" alt="image-20200516102509994" style="zoom:50%;" />

> 标注问题

> 分类问题的推广，也是复杂的结构预测的简单形式
>
> 学习模型，对观测序列给出标记序列作为预测

<img src="https://i.loli.net/2020/05/16/LX8V7zs2rdEN9iC.png" alt="image-20200516161934454" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/16/L1v8o2Wi3Y7hZsz.png" alt="image-20200516162012092" style="zoom:50%;" />

> 回归问题
>
> 输入变量、输出变量之间关系（连续）
>
> 模型为函数

> 最小二乘法求解



### 第2章 感知机

> 二分类的线性分类模型

> 输出取+1，-1

#### 感知机模型

> 判别模型y=f(x) or p(y|x)

![image-20200516170328616](https://i.loli.net/2020/05/16/ZFPLovzfiBdIqgO.png)

> w是超平面法向量，b是截距
>
> 超平面S的正负两侧
>
> w*x+b=0是分割线

<img src="C:%5CUsers%5Czhang%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200516230218294.png" alt="image-20200516230218294" style="zoom:50%;" />

#### 学习策略

> 计算任意一点x0到超平面的距离

<img src="https://i.loli.net/2020/05/16/HrAuC96SXOYJaZd.png" alt="image-20200516231839969" style="zoom:50%;" />

> 误分类点距离计算
>
> 让距离为正数，在前面加正号

<img src="https://i.loli.net/2020/05/16/lwz9m3XfYIdMuQS.png" alt="image-20200516232728873" style="zoom:50%;" />

> 感知机学习的经验风险函数
>
> M为误分类点集合

![image-20200516233210349](https://i.loli.net/2020/05/16/iso6wkfT5NCG7AF.png)

#### 感知机学习算法

> 损失极小化，求w、b的解

> 梯度下降

![image-20200516234614968](https://i.loli.net/2020/05/16/6wBaqZvec2oiJPW.png)

> 在M中随机选取一个点，进行梯度下降

![image-20200516234919446](https://i.loli.net/2020/05/16/4cvQHWfUZ672Jol.png)

#### 感知学习对偶形式

> 1. 通过观察梯度下降的方法，可以得到对偶形式的更新方法
> 2. 样本总数N作为一个整体，在更新过程中不参与

<img src="https://i.loli.net/2020/05/18/3gYadI7c1mE8ZbO.png" alt="image-20200518125814626" style="zoom:50%;" />



> 更新过程
>
> 内积直接取用

<img src="https://i.loli.net/2020/05/18/xmPJcGh1Q2LSB8A.png" alt="image-20200518130010451" style="zoom:50%;" />



### 第3章 k近邻法

> k-nearest neighbor,k-NN
>
> 分类与回归方法
>
> k近邻法不具备显示的学习过程
>
> 三个基本要素：k值的选择、距离度量、分类决策规则



#### k近邻算法

<img src="https://i.loli.net/2020/05/21/sF3dVTj2zDkRQx9.png" alt="image-20200521093854952" style="zoom:50%;" />

> 在训练集中找到与输入相近的k个实例，k个实例中多数类，判定为输入类

- 根据给定的距离度量，找到与x邻近的k个点，k为x领域，表示如下：
  $$
  N_k(x)
  $$

- 根据决策规则决定x的类别y：

  ![image-20200521094032915](https://i.loli.net/2020/05/21/yzqm7gujiAYWfdv.png)

- k近邻没有显示的学习

> 距离度量

![image-20200521155555416](https://i.loli.net/2020/05/21/MYahVNSQTouAKb9.png)

<img src="https://i.loli.net/2020/05/21/gmDE7kHFdzS9vbs.png" alt="image-20200521155706892" style="zoom:50%;" />

#### 估计误差、近似误差

> 估计误差小，测试集预测准确
>
> 近似误差小，训练集过拟合

![image-20200521184134395](https://i.loli.net/2020/05/21/wtujzHCKNTi2aW7.png)



#### kd树

> 实现k近邻，主要是对训练集进行快速的k近邻搜索
>
> kd树是二叉树



#### 平衡kd树

<img src="https://i.loli.net/2020/05/21/sWKDN2h6t5IAPBg.png" alt="image-20200521192134581" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/21/a5y46tcTHniZ7rk.png" alt="image-20200521192204089" style="zoom:50%;" />



### 第4章 朴素贝叶斯法

> 基于贝叶斯定理、特征条件独立假设
>
> 1. 特征独立假设学习输入输出联合概率分布
> 2. 对给定x，利用贝叶斯定理求后验概率最大输出y

> 朴素贝叶斯法的学习与分类、朴素贝叶斯法的参数估计



#### 朴素贝叶斯法的学习与分类

> 先学习Y的参数空间，先验分布

<img src="https://i.loli.net/2020/05/21/YG8kSO34EeIhlCQ.png" alt="image-20200521231833371" style="zoom:50%;" />

> 得到条件概率分布

<img src="https://i.loli.net/2020/05/21/shA3gTqJl7U4vR1.png" alt="image-20200521232024413" style="zoom:50%;" />

> 不做条件独立假设，需要计算的参数会很多：

<img src="https://i.loli.net/2020/05/22/VsSwKMYOTm9JBjt.png" alt="image-20200521233222143" style="zoom:50%;" />

> 朴素贝叶斯对条件概率分布作了，做了条件独立性假设

![image-20200521233339185](https://i.loli.net/2020/05/21/dbhicC9OtSEkU4n.png)

> 用来分类的特征，在类确定的条件下都是条件独立的（牺牲一定准确率）

<img src="https://i.loli.net/2020/05/22/rlZxnL9YSWHI5oE.png" alt="image-20200521234317736" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/21/e2iycxKzU5mNPFq.png" alt="image-20200521234348573" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/21/k5JQsNw8Zzo2Fve.png" alt="image-20200521234624590" style="zoom:50%;" />

> 后验概率最大化



<img src="https://i.loli.net/2020/05/21/4PtagmBMuo7XLv3.png" alt="image-20200521235707960" style="zoom: 67%;" />

> L函数值为0、1，求f(x)最小的值
>
> 也就是求y！=ck时，f(x)最小的概率

![image-20200521235758293](https://i.loli.net/2020/05/21/gYpzucyoZN8HlQC.png)

#### 朴素贝叶斯法的参数估计

> 极大似然估计：

##### **朴素贝叶斯学习：**

<img src="https://i.loli.net/2020/05/22/edMyP6sOHl2f9FY.png" alt="image-20200522111249976" style="zoom:50%;" />

##### **先验概率P(Y=ck)的极大似然估计：**

<img src="https://i.loli.net/2020/05/22/CFyolHU9SKePgfn.png" alt="image-20200522111459359" style="zoom:50%;" />

##### **贝叶斯估计**

> 极大似然估计会出现概率值为0
>
> 随机变量各个频数上赋值一个“拉姆达”

<img src="https://i.loli.net/2020/05/22/DuFCjbsPNBrmZfS.png" alt="image-20200522113153614" style="zoom: 50%;" />

### 第5章 决策树

> 基本的分类与回归方法
>
> 利用训练数据，根据损失函数最小原则建立决策树模型
>
> 特征选择、决策树生成、决策树修剪