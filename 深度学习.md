# 深度学习



## 概率论与数理统计



#### 联合概率、边缘概率、条件概率

> - 联合概率P(X=a,Y=b)
>   满足X=a且Y=b的面积
> - 边缘概率P(X=a)
>   不考虑Y的取值，所有满足X=a的区域的总面积
> - 条件概率P(X=a|Y=b)
>   在Y=b的前提下，满足X=a的面积（比例）

![img](https://i.loli.net/2020/05/12/tsFICnL9QKkchHP.jpg)联

> 联合概率

$$
P(X=红色,Y=数字牌)=3/16
$$

> 边缘概率

$$
P(X=红色)=9/16
$$

> 条件概率

$$
P(Y=数字牌|X=红色)=3/9=1/3
$$



#### 似然概率

> 对于函数![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))，从不同的观测角度来看可以分为以下两种情况：
>
> - 如果![\theta](https://math.jianshu.com/math?formula=%5Ctheta)已知且保持不变，![x](https://math.jianshu.com/math?formula=x)是变量，则![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))称为概率函数，表示不同![x](https://math.jianshu.com/math?formula=x)出现的概率。
> - 如果![x](https://math.jianshu.com/math?formula=x)已知且保持不变，![\theta](https://math.jianshu.com/math?formula=%5Ctheta)是变量，则![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))称为似然函数，表示不同![\theta](https://math.jianshu.com/math?formula=%5Ctheta)下，![x](https://math.jianshu.com/math?formula=x)出现的概率，也记作![L(\theta|x)](https://math.jianshu.com/math?formula=L(%5Ctheta%7Cx))或![L(x;\theta)](https://math.jianshu.com/math?formula=L(x%3B%5Ctheta))或![f(https://math.jianshu.com/math?formula=f(x%3B\theta))](https://math.jianshu.com/math?formula=f(x%3B%5Ctheta))。





















## 统计学方法

### 第1章统计学习及监督 学习概论

#### 统计学的方法

> 组成：监督学习supervised learning，无监督学习unsupervised learning，强化学习reinforcement learning

> 过程：
>
> 1. 给定数据，假设数据独立同分布；
> 2. 假设学习模型属于某函数集合（假设空间）；
> 3. 应用评价标准，在假设空间中选取最优模型；

> 三要素：模型model，策略strategy，算法algorithm
>
> 步骤：1. 确定学习模型集合  2. 确定策略（模型选择）方法  3. 用算法求解最优模型



#### 监督学习（变量连续、离散）

> 通过标注数据数据，给出输入输出的关系
>
> 本质：学习输入到输出的映射的统计规律

> 输入变量与输出变量连续———称为回归问题
>
> 输出变量为有限个离散变量———称为分类问题
>
> 输入和输出变量都为变量序列的预测———称为标注问题



#### 模型分类（生成模型、判别模型）

> 概率模型取条件概率分布形式

$$
P(Y/X)
$$

> 非概率模型取函数形式

$$
y=f(x)
$$

> 概率模型为联合概率分布，称为生成模型
>
> 非概率模型为条件概率分布或决策函数，称为判别模型

> 假设有四个samples： 

![img](https://i.loli.net/2020/05/11/riMRBUekbvhqO21.jpg)



> 生成式模型的世界是这个样子：朴素贝叶斯法和隐马尔可夫模型

![img](https://i.loli.net/2020/05/11/9FEgT4Lks8AylSa.jpg)
![[公式]](https://www.zhihu.com/equation?tex=\Sigma+P(x%2C+y)+%3D+1)

```
## 数据学习p(x,y)联合概率分布，从而求出p(y|x)的条件概率分布作为预测模型
```



> 而判定式模型的世界是这个样子：k近邻、感知机、决策树、逻辑回归、最大熵、支持向量机、提升方法、条件随机场
>
> 在x成立的条件下，y的概率

![img](https://i.loli.net/2020/05/11/FqmxQ7SCzZKAljc.jpg)![[公式]](https://www.zhihu.com/equation?tex=\sum_{y}{P(y+|+x)}+%3D+1+)

```
## 直接学习决策函数y=f(x)或者条件概率分布P(Y|X)作为预测模型
```



#### 算法分类

> 在线学习：每次接受一个样本，预测，学习模型，不断重复
>
> 批量学习：一次接受所有样本，学习模型，进行预测

<img src="https://i.loli.net/2020/05/11/bvH2xo9BAhl5rCq.png" alt="image-20200511154313619" style="zoom:50%;" />



#### 技巧分类（贝叶斯、极大似然、核方法）

> 贝叶斯学习
>
> 朴素贝叶斯、潜在狄利克雷分配

<img src="https://i.loli.net/2020/05/11/C3fniYuqWQXVo2g.png" alt="image-20200511155758680" style="zoom: 50%;" />

> 核方法
>
> 核PCA、核k均值、核函数支持向量机

> **把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。**



#### 假设空间（策略在此中选取最优模型）

> 决策函数表示的为非概率模型
>
> 条件概率表示的为概率模型

![image-20200512213925671](https://i.loli.net/2020/05/12/TzASwVu4EFB5KGC.png)



#### 策略

> 损失函数度量模型一次预测好坏
>
> 风险函数度量平均意义下模型好坏

<img src="https://i.loli.net/2020/05/12/eFupKy1ioDxr8hG.png" alt="image-20200512214819190" style="zoom:50%;" />

<img src="C:%5CUsers%5Czhang%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200512214931661.png" alt="image-20200512214931661" style="zoom:50%;" />



> 联合分布P(x,y)未知，假设P(x,y)已知就可以直接求出P(y|x)，没必要学习

![image-20200512215656217](https://i.loli.net/2020/05/12/Cc4wWOqva7ztn2r.png)



> 再加上正则化项
>
> 监督学习问题称为经验函数的最优化问题

![image-20200512220028657](https://i.loli.net/2020/05/12/wpRSVcQX9WajzAv.png)