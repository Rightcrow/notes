# 深度学习



## 概率论与数理统计



#### 联合概率、边缘概率、条件概率

> - 联合概率P(X=a,Y=b)
>   满足X=a且Y=b的面积
> - 边缘概率P(X=a)
>   不考虑Y的取值，所有满足X=a的区域的总面积
> - 条件概率P(X=a|Y=b)
>   在Y=b的前提下，满足X=a的面积（比例）

![img](https://i.loli.net/2020/05/12/tsFICnL9QKkchHP.jpg)联

> 联合概率

$$
P(X=红色,Y=数字牌)=3/16
$$

> 边缘概率

$$
P(X=红色)=9/16
$$

> 条件概率

$$
P(Y=数字牌|X=红色)=3/9=1/3
$$



#### 似然概率

> 对于函数![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))，从不同的观测角度来看可以分为以下两种情况：
>
> - 如果![\theta](https://math.jianshu.com/math?formula=%5Ctheta)已知且保持不变，![x](https://math.jianshu.com/math?formula=x)是变量，则![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))称为概率函数，表示不同![x](https://math.jianshu.com/math?formula=x)出现的概率。
> - 如果![x](https://math.jianshu.com/math?formula=x)已知且保持不变，![\theta](https://math.jianshu.com/math?formula=%5Ctheta)是变量，则![P(x|\theta)](https://math.jianshu.com/math?formula=P(x%7C%5Ctheta))称为似然函数，表示不同![\theta](https://math.jianshu.com/math?formula=%5Ctheta)下，![x](https://math.jianshu.com/math?formula=x)出现的概率，也记作![L(\theta|x)](https://math.jianshu.com/math?formula=L(%5Ctheta%7Cx))或![L(x;\theta)](https://math.jianshu.com/math?formula=L(x%3B%5Ctheta))或![f(https://math.jianshu.com/math?formula=f(x%3B\theta))](https://math.jianshu.com/math?formula=f(x%3B%5Ctheta))。





















## 统计学方法

### 第1章 统计学习及监督 学习概论

#### 统计学的方法（监督、无监督）

> 组成：监督学习supervised learning，无监督学习unsupervised learning，强化学习reinforcement learning

> 过程：
>
> 1. 给定数据，假设数据独立同分布；
> 2. 假设学习模型属于某函数集合（假设空间）；
> 3. 应用评价标准，在假设空间中选取最优模型；

> 三要素：模型model，策略strategy，算法algorithm
>
> 步骤：1. 确定学习模型集合  2. 确定策略（模型选择）方法  3. 用算法求解最优模型



#### 监督学习（变量连续、离散）

> 通过标注数据数据，给出输入输出的关系
>
> 本质：学习输入到输出的映射的统计规律

> 输入变量与输出变量连续———称为回归问题
>
> 输出变量为有限个离散变量———称为分类问题
>
> 输入和输出变量都为变量序列的预测———称为标注问题



#### 模型分类（生成模型、判别模型）

> 概率模型取条件概率分布形式

$$
P(Y/X)
$$

> 非概率模型取函数形式

$$
y=f(x)
$$

> 概率模型为联合概率分布，称为生成模型
>
> 非概率模型为条件概率分布或决策函数，称为判别模型

> 假设有四个samples： 

![img](https://i.loli.net/2020/05/11/riMRBUekbvhqO21.jpg)



> 生成式模型的世界是这个样子：朴素贝叶斯法和隐马尔可夫模型

![img](https://i.loli.net/2020/05/11/9FEgT4Lks8AylSa.jpg)
![[公式]](https://www.zhihu.com/equation?tex=\Sigma+P(x%2C+y)+%3D+1)

```
## 数据学习p(x,y)联合概率分布，从而求出p(y|x)的条件概率分布作为预测模型
```



> 而判定式模型的世界是这个样子：k近邻、感知机、决策树、逻辑回归、最大熵、支持向量机、提升方法、条件随机场
>
> 在x成立的条件下，y的概率

![img](https://i.loli.net/2020/05/11/FqmxQ7SCzZKAljc.jpg)![[公式]](https://www.zhihu.com/equation?tex=\sum_{y}{P(y+|+x)}+%3D+1+)

```
## 直接学习决策函数y=f(x)或者条件概率分布P(Y|X)作为预测模型
```



#### 算法分类

> 在线学习：每次接受一个样本，预测，学习模型，不断重复
>
> 批量学习：一次接受所有样本，学习模型，进行预测

<img src="https://i.loli.net/2020/05/11/bvH2xo9BAhl5rCq.png" alt="image-20200511154313619" style="zoom:50%;" />



#### 技巧分类（贝叶斯、极大似然、核方法）

> 贝叶斯学习
>
> 朴素贝叶斯、潜在狄利克雷分配

<img src="https://i.loli.net/2020/05/11/C3fniYuqWQXVo2g.png" alt="image-20200511155758680" style="zoom: 50%;" />

> 核方法
>
> 核PCA、核k均值、核函数支持向量机

> **把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。**



#### 假设空间（策略在此中选取最优模型）

> 决策函数表示的为非概率模型
>
> 条件概率表示的为概率模型

![image-20200512213925671](https://i.loli.net/2020/05/12/TzASwVu4EFB5KGC.png)



#### 策略（期望损失、经验损失）

> 损失函数度量模型一次预测好坏
>
> 风险函数度量平均意义下模型好坏

<img src="https://i.loli.net/2020/05/12/eFupKy1ioDxr8hG.png" alt="image-20200512214819190" style="zoom:50%;" />

<img src="C:%5CUsers%5Czhang%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200512214931661.png" alt="image-20200512214931661" style="zoom:50%;" />



> 联合分布P(x,y)未知，假设P(x,y)已知就可以直接求出P(y|x)，没必要学习

![image-20200512215656217](https://i.loli.net/2020/05/12/Cc4wWOqva7ztn2r.png)



> 再加上正则化项
>
> 监督学习问题称为经验函数的最优化问题

![image-20200512220028657](https://i.loli.net/2020/05/12/wpRSVcQX9WajzAv.png)



#### 泛化误差上界

> 样本越大泛化误差上界趋于0
>
> 假设空间越大，泛化误差上界越大



#### 生成模型

> 通过联合概率分布p(x,y)，求出条件概率分布p(y|x)作为预测模型
>
> 生成模型如下：

![image-20200515211114721](https://i.loli.net/2020/05/15/vjCF1PQVlLpaYDg.png)

```
# 优点：1.可以还原联合分布。
       2.收敛快，样本容量增加更快收敛于真实模型。
       3.存在隐变量时，仍可以用，但判别模型不行。
```



#### 判别模型

> 直接学习决策函数或者条件概率作为预测模型
>
> 判别方法关心，给定x，应该预测怎样的y



#### 监督学习问题（分类、标注、回归；精确率、召回率）

> 分类问题
>
> 输出Y为有限个离散值
>
> 从数据中学习一个分类模型或决策函数，称为分类器

<img src="https://i.loli.net/2020/05/16/MVwN9a4CtqT1Odn.png" alt="image-20200516102436786" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/16/4JYTNdePgk5vKOi.png" alt="image-20200516102509994" style="zoom:50%;" />

> 标注问题

> 分类问题的推广，也是复杂的结构预测的简单形式
>
> 学习模型，对观测序列给出标记序列作为预测

<img src="https://i.loli.net/2020/05/16/LX8V7zs2rdEN9iC.png" alt="image-20200516161934454" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/16/L1v8o2Wi3Y7hZsz.png" alt="image-20200516162012092" style="zoom:50%;" />

> 回归问题
>
> 输入变量、输出变量之间关系（连续）
>
> 模型为函数

> 最小二乘法求解



### 第2章 感知机

> 二分类的线性分类模型

> 输出取+1，-1

#### 感知机模型

> 判别模型y=f(x) or p(y|x)

![image-20200516170328616](https://i.loli.net/2020/05/16/ZFPLovzfiBdIqgO.png)

> w是超平面法向量，b是截距
>
> 超平面S的正负两侧
>
> w*x+b=0是分割线

<img src="C:%5CUsers%5Czhang%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200516230218294.png" alt="image-20200516230218294" style="zoom:50%;" />

#### 学习策略

> 计算任意一点x0到超平面的距离

<img src="https://i.loli.net/2020/05/16/HrAuC96SXOYJaZd.png" alt="image-20200516231839969" style="zoom:50%;" />

> 误分类点距离计算
>
> 让距离为正数，在前面加正号

<img src="https://i.loli.net/2020/05/16/lwz9m3XfYIdMuQS.png" alt="image-20200516232728873" style="zoom:50%;" />

> 感知机学习的经验风险函数
>
> M为误分类点集合

![image-20200516233210349](https://i.loli.net/2020/05/16/iso6wkfT5NCG7AF.png)

#### 感知机学习算法

> 损失极小化，求w、b的解

> 梯度下降

![image-20200516234614968](https://i.loli.net/2020/05/16/6wBaqZvec2oiJPW.png)

> 在M中随机选取一个点，进行梯度下降

![image-20200516234919446](https://i.loli.net/2020/05/16/4cvQHWfUZ672Jol.png)

#### 感知学习对偶形式

> 1. 通过观察梯度下降的方法，可以得到对偶形式的更新方法
> 2. 样本总数N作为一个整体，在更新过程中不参与

<img src="https://i.loli.net/2020/05/18/3gYadI7c1mE8ZbO.png" alt="image-20200518125814626" style="zoom:50%;" />



> 更新过程
>
> 内积直接取用

<img src="https://i.loli.net/2020/05/18/xmPJcGh1Q2LSB8A.png" alt="image-20200518130010451" style="zoom:50%;" />